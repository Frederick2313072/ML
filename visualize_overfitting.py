"""
å¯è§†åŒ–AdaBoostè¿‡æ‹Ÿåˆè¿‡ç¨‹
ç®€æ´çš„è„šæœ¬ï¼Œå±•ç¤ºæ¨¡å‹éšç€å¼±å­¦ä¹ å™¨æ•°é‡å¢åŠ çš„è¿‡æ‹Ÿåˆè¡Œä¸º

å¯é€‰åŠŸèƒ½ï¼šå¯ç”¨è¯¦ç»†è®­ç»ƒç›‘æ§ï¼ˆå‚è€ƒ docs/monitor.mdï¼‰
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from src.utils import prepare_data
from src.evaluation import visualize_overfitting_process
from src.monitor import BoostMonitor
from src.patch import AdaBoostClfWithMonitor


def visualize_monitor_data(monitor, n_estimators, is_noisy):
    """
    å¯è§†åŒ– BoostMonitor è®°å½•çš„è®­ç»ƒæ•°æ®
    å‚è€ƒ docs/monitor.md ä¸­çš„æ•°æ®ç»“æ„
    
    ç”Ÿæˆ 6 ä¸ªå­å›¾ï¼š
    1. é”™è¯¯ç‡æ¼”åŒ–ï¼ˆweighted vs unweightedï¼‰
    2. Alpha ç³»æ•°æ¼”åŒ–
    3. è®­ç»ƒ vs éªŒè¯å‡†ç¡®ç‡
    4. å™ªå£°æ ·æœ¬ vs å¹²å‡€æ ·æœ¬æƒé‡ï¼ˆä»…å™ªå£°æ•°æ®ï¼‰
    5. F1 åˆ†æ•°æ¼”åŒ–
    6. æ ·æœ¬æƒé‡åˆ†å¸ƒå˜åŒ–
    """
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(f'Detailed Training Monitoring (n_estimators={n_estimators})', 
                 fontsize=16, fontweight='bold')
    
    rounds = list(range(1, len(monitor.error_history) + 1))
    
    # 1. é”™è¯¯ç‡æ¼”åŒ–
    ax1 = axes[0, 0]
    ax1.plot(rounds, monitor.error_history, 'b-', linewidth=2, label='Weighted Error')
    if len(monitor.error_without_weight_history) == len(rounds):
        ax1.plot(rounds, monitor.error_without_weight_history, 'r--', 
                linewidth=2, label='Unweighted Error', alpha=0.7)
    ax1.set_xlabel('Boosting Round')
    ax1.set_ylabel('Error Rate')
    ax1.set_title('Error Rate Evolution')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Alpha ç³»æ•°
    ax2 = axes[0, 1]
    ax2.plot(rounds, monitor.alpha_history, 'g-', linewidth=2, marker='o', markersize=4)
    ax2.axhline(y=np.mean(monitor.alpha_history), color='orange', linestyle='--', 
               label=f'Mean={np.mean(monitor.alpha_history):.3f}', alpha=0.7)
    ax2.set_xlabel('Boosting Round')
    ax2.set_ylabel('Alpha (Weak Learner Weight)')
    ax2.set_title('Alpha Coefficient Evolution')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å‡†ç¡®ç‡
    ax3 = axes[0, 2]
    if len(monitor.acc_on_train_data) > 0:
        ax3.plot(rounds, monitor.acc_on_train_data, 'b-', linewidth=2, 
                label='Train Accuracy', marker='o', markersize=4)
    if len(monitor.val_acc_history) > 0:
        ax3.plot(rounds, monitor.val_acc_history, 'r-', linewidth=2, 
                label='Val Accuracy', marker='s', markersize=4)
    ax3.set_xlabel('Boosting Round')
    ax3.set_ylabel('Accuracy')
    ax3.set_title('Accuracy Evolution')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. å™ªå£° vs å¹²å‡€æ ·æœ¬æƒé‡
    ax4 = axes[1, 0]
    if is_noisy and len(monitor.noisy_weight_history) > 0:
        ax4.plot(rounds, monitor.noisy_weight_history, 'r-', linewidth=2, 
                label='Noisy Samples', marker='o', markersize=4)
        ax4.plot(rounds, monitor.clean_weight_history, 'g-', linewidth=2, 
                label='Clean Samples', marker='s', markersize=4)
        ax4.axhline(y=0.5, color='black', linestyle='--', alpha=0.3, linewidth=1)
        ax4.set_xlabel('Boosting Round')
        ax4.set_ylabel('Total Weight')
        ax4.set_title('Noisy vs Clean Sample Weights')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'N/A\n(Clean Data)', 
                ha='center', va='center', fontsize=14, color='gray', 
                transform=ax4.transAxes)
        ax4.set_xticks([])
        ax4.set_yticks([])
    
    # 5. F1 åˆ†æ•°
    ax5 = axes[1, 1]
    if len(monitor.f1_on_training_data) > 0:
        ax5.plot(rounds, monitor.f1_on_training_data, 'b-', linewidth=2, 
                label='Train F1', marker='o', markersize=4)
    if len(monitor.val_f1_history) > 0:
        ax5.plot(rounds, monitor.val_f1_history, 'r-', linewidth=2, 
                label='Val F1', marker='s', markersize=4)
    ax5.set_xlabel('Boosting Round')
    ax5.set_ylabel('F1 Score')
    ax5.set_title('F1 Score Evolution')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. æ ·æœ¬æƒé‡åˆ†å¸ƒ
    ax6 = axes[1, 2]
    if len(monitor.sample_weights_history) > 0:
        # é€‰æ‹©å…³é”®è½®æ¬¡
        key_rounds = [0, len(rounds)//3, len(rounds)*2//3, len(rounds)-1]
        positions = []
        data_to_plot = []
        labels = []
        
        for i, idx in enumerate(key_rounds):
            if idx < len(monitor.sample_weights_history):
                positions.append(i + 1)
                data_to_plot.append(monitor.sample_weights_history[idx])
                labels.append(f'R{idx+1}')
        
        bp = ax6.boxplot(data_to_plot, positions=positions, widths=0.6, 
                        patch_artist=True, labels=labels)
        for patch in bp['boxes']:
            patch.set_facecolor('lightblue')
            patch.set_alpha(0.7)
        
        ax6.set_ylabel('Sample Weight')
        ax6.set_title('Sample Weight Distribution')
        ax6.grid(True, alpha=0.3, axis='y')
    else:
        ax6.text(0.5, 0.5, 'N/A', ha='center', va='center', 
                fontsize=14, color='gray', transform=ax6.transAxes)
        ax6.set_xticks([])
        ax6.set_yticks([])
    
    plt.tight_layout()
    plt.show()
    plt.close()


def main():
    """ä¸»å‡½æ•°ï¼šå¯è§†åŒ–è¿‡æ‹Ÿåˆè¿‡ç¨‹"""

    print("\n" + "â–ˆ" * 60)
    print("AdaBoost è¿‡æ‹Ÿåˆå¯è§†åŒ–".center(56))
    print("â–ˆ" * 60)

    # ========== 1. é€‰æ‹©æ•°æ®ç±»å‹ ==========
    print("\né€‰æ‹©æ•°æ®ç±»å‹:")
    print("1. å¹²å‡€æ•°æ®ï¼ˆæ— å™ªå£°ï¼‰")
    print("2. å«å™ªå£°æ•°æ®ï¼ˆ5%å™ªå£°ï¼‰")
    print("3. å«å™ªå£°æ•°æ®ï¼ˆ10%å™ªå£°ï¼‰")

    # é»˜è®¤ä½¿ç”¨é€‰é¡¹2
    choice = 2  # å¯ä»¥ä¿®æ”¹ä¸º1æˆ–3

    if choice == 1:
        noise_ratio = 0
        data_type = "å¹²å‡€æ•°æ®"
    elif choice == 2:
        noise_ratio = 0.05
        data_type = "5%å™ªå£°æ•°æ®"
    else:
        noise_ratio = 0.10
        data_type = "10%å™ªå£°æ•°æ®"

    print(f"\nä½¿ç”¨: {data_type}")
    print("-" * 60)

    # ========== 2. å‡†å¤‡æ•°æ® ==========
    print("\nå‡†å¤‡æ•°æ®...")
    X_train, X_test, y_train, y_test, noise_idx, clean_idx = prepare_data(
        noise_ratio=noise_ratio
    )

    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    if noise_ratio > 0:
        print(f"å™ªå£°æ ·æœ¬: {len(noise_idx)}")
        print(f"å¹²å‡€æ ·æœ¬: {len(clean_idx)}")

    # ========== 3. é€‰æ‹©é…ç½® ==========
    print("\n" + "=" * 60)
    print("é…ç½®é€‰é¡¹".center(56))
    print("=" * 60)

    # é…ç½®1: å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èï¼‰
    config = {
        "base_estimator": DecisionTreeClassifier(max_depth=1),  # å†³ç­–æ ‘æ¡©
        "n_estimators_list": [1, 5, 10, 20, 30, 40, 50, 75, 100],  # æµ‹è¯•ç‚¹
        "learning_rate": 0.5,  # å­¦ä¹ ç‡
        "random_state": 42,
    }

    # é…ç½®2: ç²¾ç»†åˆ†æï¼ˆæ›´å¤šæµ‹è¯•ç‚¹ï¼Œéœ€è¦æ›´é•¿æ—¶é—´ï¼‰
    # config = {
    #     "base_estimator": DecisionTreeClassifier(max_depth=1),
    #     "n_estimators_list": list(range(1, 101, 5)),  # [1, 6, 11, ..., 96]
    #     "learning_rate": 0.5,
    #     "random_state": 42,
    # }

    # é…ç½®3: æ·±æ ‘æµ‹è¯•ï¼ˆè§‚å¯Ÿæ›´å¤æ‚åŸºå­¦ä¹ å™¨çš„å½±å“ï¼‰
    # config = {
    #     "base_estimator": DecisionTreeClassifier(max_depth=3),
    #     "n_estimators_list": [1, 5, 10, 20, 30, 40, 50],
    #     "learning_rate": 0.5,
    #     "random_state": 42,
    # }

    print(f"åŸºå­¦ä¹ å™¨: å†³ç­–æ ‘ (max_depth={config['base_estimator'].max_depth})")
    print(f"æµ‹è¯•ç‚¹æ•°é‡: {len(config['n_estimators_list'])}")
    print(f"å¼±å­¦ä¹ å™¨èŒƒå›´: {config['n_estimators_list'][0]} - {config['n_estimators_list'][-1]}")
    print(f"å­¦ä¹ ç‡: {config['learning_rate']}")

    # ========== 4. å¯è§†åŒ–è¿‡æ‹Ÿåˆ ==========
    print("\nå¼€å§‹è®­ç»ƒå’Œå¯è§†åŒ–...")
    print("-" * 60)

    results = visualize_overfitting_process(
        X_train,
        y_train,
        X_test,
        y_test,
        base_estimator=config["base_estimator"],
        n_estimators_list=config["n_estimators_list"],
        learning_rate=config["learning_rate"],
        random_state=config["random_state"],
        save_path=None,  # è®¾ä¸ºè·¯å¾„å¯ä¿å­˜å›¾è¡¨ï¼Œå¦‚ 'overfitting.png'
    )

    # ========== 5. é¢å¤–åˆ†æï¼ˆå¯é€‰ï¼‰ ==========
    print("\n" + "=" * 60)
    print("å»ºè®®".center(56))
    print("=" * 60)

    best_idx = results["test_accuracy"].index(max(results["test_accuracy"]))
    best_n = results["n_estimators"][best_idx]
    final_n = results["n_estimators"][-1]
    final_overfit = results["overfitting_degree"][-1]

    # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
    if final_overfit > 0.15:
        print("\nâš ï¸  ä¸¥é‡è¿‡æ‹Ÿåˆè­¦å‘Š:")
        print(f"   - å½“å‰è¿‡æ‹Ÿåˆç¨‹åº¦: {final_overfit:.2%}")
        print(f"   - å»ºè®®å‡å°‘å¼±å­¦ä¹ å™¨æ•°é‡è‡³ {best_n} å·¦å³")
        print(f"   - æˆ–ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼ˆå¦‚ 0.1ï¼‰")
    elif final_overfit > 0.10:
        print("\nâš ï¸  ä¸­åº¦è¿‡æ‹Ÿåˆ:")
        print(f"   - å½“å‰è¿‡æ‹Ÿåˆç¨‹åº¦: {final_overfit:.2%}")
        print(f"   - å»ºè®®ä½¿ç”¨æ—©åœï¼Œåœ¨ n={best_n} å¤„åœæ­¢è®­ç»ƒ")
    elif final_overfit < 0.05:
        print("\nâœ“ æ¨¡å‹æ‹Ÿåˆè‰¯å¥½:")
        print(f"   - è¿‡æ‹Ÿåˆç¨‹åº¦ä½: {final_overfit:.2%}")
        print(f"   - å¯ä»¥è€ƒè™‘å¢åŠ å¼±å­¦ä¹ å™¨æ•°é‡ä»¥æå‡æ€§èƒ½")
    else:
        print("\nâœ“ æ¨¡å‹è¡¨ç°è‰¯å¥½:")
        print(f"   - è¿‡æ‹Ÿåˆç¨‹åº¦: {final_overfit:.2%} (å¯æ¥å—)")
        print(f"   - å»ºè®®ä½¿ç”¨ n={best_n} ä¸ªå¼±å­¦ä¹ å™¨")

    # å™ªå£°æ•°æ®çš„é¢å¤–å»ºè®®
    if noise_ratio > 0:
        print(f"\nğŸ’¡ å™ªå£°æ•°æ®å»ºè®®:")
        print(f"   - å½“å‰æ•°æ®æœ‰ {noise_ratio*100:.0f}% å™ªå£°")
        print(f"   - AdaBoost å¯¹å™ªå£°æ•æ„Ÿï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ")
        print(f"   - å»ºè®®:")
        print(f"     1. ä½¿ç”¨è¾ƒå°‘çš„å¼±å­¦ä¹ å™¨ï¼ˆ{best_n} å·¦å³ï¼‰")
        print(f"     2. é™ä½å­¦ä¹ ç‡ï¼ˆä» 0.5 åˆ° 0.3ï¼‰")
        print(f"     3. è€ƒè™‘æ•°æ®æ¸…æ´—æˆ–å™ªå£°é²æ£’æ–¹æ³•")

    # ========== 6. å¯é€‰ï¼šè¯¦ç»†è®­ç»ƒç›‘æ§ ==========
    # å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥å¯ç”¨è¯¦ç»†ç›‘æ§å¯è§†åŒ–
    enable_detailed_monitoring = False  # è®¾ä¸º True å¯ç”¨è¯¦ç»†ç›‘æ§
    
    if enable_detailed_monitoring:
        print("\n" + "=" * 60)
        print("è¯¦ç»†è®­ç»ƒç›‘æ§".center(56))
        print("=" * 60)
        print(f"\né‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹ (n={best_n})ï¼Œå¯ç”¨ç›‘æ§...")
        
        # åˆ›å»ºç›‘æ§å™¨ï¼ˆå‚è€ƒ docs/monitor.mdï¼‰
        monitor = BoostMonitor(
            noise_indices=noise_idx,
            clean_indices=clean_idx,
            is_data_noisy=(noise_ratio > 0),
            checkpoint_interval=999,
            checkpoint_prefix="temp"
        )
        
        # ä½¿ç”¨ç›‘æ§å™¨è®­ç»ƒ
        clf_monitored = AdaBoostClfWithMonitor(
            estimator=config["base_estimator"],
            n_estimators=best_n,
            learning_rate=config["learning_rate"],
            random_state=config["random_state"],
            monitor=monitor
        )
        clf_monitored.fit(X_train, y_train)
        
        # ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–ï¼ˆ6ä¸ªå­å›¾ï¼‰
        print("\nç”Ÿæˆè¯¦ç»†è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–...")
        visualize_monitor_data(monitor, best_n, noise_ratio > 0)
        
        print("\nâœ“ è¯¦ç»†ç›‘æ§å¯è§†åŒ–å®Œæˆï¼")
        print("\nğŸ“Š ç›‘æ§æ•°æ®åŒ…å«:")
        print(f"   - é”™è¯¯ç‡å†å²: {len(monitor.error_history)} è½®")
        print(f"   - Alphaç³»æ•°: {len(monitor.alpha_history)} è½®")
        print(f"   - æ ·æœ¬æƒé‡æ¼”åŒ–: {len(monitor.sample_weights_history)} è½®")
    
    print("\n" + "=" * 60)
    print("\nâœ“ å¯è§†åŒ–å®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("   - å›¾è¡¨ä¼šè‡ªåŠ¨æ˜¾ç¤ºï¼ˆå…³é—­çª—å£ç»§ç»­ï¼‰")
    print("   - è¦ä¿å­˜å›¾è¡¨ï¼Œè®¾ç½® save_path='overfitting.png'")
    print("   - è¦æµ‹è¯•ä¸åŒé…ç½®ï¼Œä¿®æ”¹è„šæœ¬ä¸­çš„ config å­—å…¸")
    print(f"   - è¦å¯ç”¨è¯¦ç»†ç›‘æ§ï¼Œè®¾ç½® enable_detailed_monitoring=True")
    print("=" * 60)


if __name__ == "__main__":
    main()


