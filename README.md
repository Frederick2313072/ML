# AdaBoost è®­ç»ƒç›‘æ§ä¸è¿‡æ‹Ÿåˆå¯è§†åŒ–é¡¹ç›®

æœ¬é¡¹ç›®ç”¨äºç ”ç©¶å’Œç›‘æ§ AdaBoost è®­ç»ƒè¿‡ç¨‹ï¼Œç‰¹åˆ«å…³æ³¨æ ‡ç­¾å™ªå£°çš„å½±å“å’Œè¿‡æ‹Ÿåˆè¡Œä¸ºã€‚

## ğŸ¯ é¡¹ç›®ç‰¹ç‚¹

âœ¨ **è®­ç»ƒç›‘æ§**ï¼šå®æ—¶è¿½è¸ª AdaBoost æ¯è½®è¿­ä»£çš„æ ·æœ¬æƒé‡å˜åŒ–  
ğŸ“Š **å®Œå–„è¯„ä¼°**ï¼šæä¾›è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯è§†åŒ–åˆ†æ  
ğŸ” **å™ªå£°åˆ†æ**ï¼šå¯¹æ¯”å™ªå£°æ ·æœ¬ä¸å¹²å‡€æ ·æœ¬çš„è®­ç»ƒè¡¨ç°  
ğŸ¯ **ç‰¹å¾é‡è¦æ€§**ï¼šå¯è§†åŒ–åˆ†æå“ªäº›åƒç´ å¯¹è¯†åˆ«æœ€é‡è¦  
ğŸ“ˆ **è¿‡æ‹Ÿåˆå¯è§†åŒ–**ï¼šç›´è§‚å±•ç¤ºæ¨¡å‹éšå¼±å­¦ä¹ å™¨æ•°é‡çš„è¿‡æ‹Ÿåˆè¿‡ç¨‹  
ğŸ›¡ï¸ **é²æ£’AdaBoost**ï¼šè§£å†³å™ªå£°æ•æ„Ÿå’Œè¿‡æ‹Ÿåˆé—®é¢˜çš„æ”¹è¿›æ–¹æ³• â­æ–°å¢  
ğŸ¨ **ä¸­æ–‡æ”¯æŒ**ï¼šæ‰€æœ‰å›¾è¡¨å’ŒæŠ¥å‘Šæ”¯æŒä¸­æ–‡æ˜¾ç¤º  

## ğŸ“ é¡¹ç›®ç»“æ„

```text
ML/
â”œâ”€â”€ src/                           # æºä»£ç æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py               # PythonåŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ evaluation.py             # è¯„ä¼°æ¨¡å—ï¼ˆå«è¿‡æ‹Ÿåˆå¯è§†åŒ–ï¼‰
â”‚   â”œâ”€â”€ monitor.py                # è®­ç»ƒç›‘æ§å™¨
â”‚   â”œâ”€â”€ patch.py                  # AdaBoostæ–¹æ³•æ‹¦æˆªè¡¥ä¸
â”‚   â”œâ”€â”€ utils.py                  # æ•°æ®å‡†å¤‡å·¥å…·
â”‚   â””â”€â”€ robust_adaboost.py        # é²æ£’AdaBoostå®ç° â­æ–°å¢
â”‚
â”œâ”€â”€ docs/                          # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ overfitting_visualization_guide.md  # è¿‡æ‹Ÿåˆå¯è§†åŒ–æŒ‡å—
â”‚   â””â”€â”€ robust_adaboost_guide.md  # é²æ£’AdaBoostä½¿ç”¨æŒ‡å— â­æ–°å¢
â”‚
â”œâ”€â”€ train_with_clean_data.py      # å¹²å‡€æ•°æ®è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_with_noise_track.py     # å«å™ªæ•°æ®è®­ç»ƒè„šæœ¬
â”œâ”€â”€ visualize_overfitting.py      # è¿‡æ‹Ÿåˆå¯è§†åŒ–è„šæœ¬
â”œâ”€â”€ demo_robust.py                # é²æ£’æ–¹æ³•æ¼”ç¤ºè„šæœ¬ â­æ–°å¢
â”œâ”€â”€ compare_robust_methods.py     # é²æ£’æ–¹æ³•å¯¹æ¯”å®éªŒ â­æ–°å¢
â”œâ”€â”€ environment.yaml              # Condaç¯å¢ƒé…ç½®
â””â”€â”€ README.md                     # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# ä½¿ç”¨conda
conda env create -f environment.yaml
conda activate machinelearning

# æˆ–ä½¿ç”¨pip
pip install numpy pandas matplotlib seaborn scikit-learn tqdm mplfonts
```

### 2. é€‰æ‹©ä½¿ç”¨æ–¹å¼

#### ğŸ›¡ï¸ æ–¹å¼Aï¼šé²æ£’AdaBoostï¼ˆæ¨èï¼è§£å†³å™ªå£°å’Œè¿‡æ‹Ÿåˆï¼‰â­æ–°å¢

```bash
# å¿«é€Ÿæ¼”ç¤º
python demo_robust.py

# å®Œæ•´å¯¹æ¯”å®éªŒ
python compare_robust_methods.py
```

**è¿™ä¼šåšä»€ä¹ˆï¼Ÿ**
- å¯¹æ¯”æ ‡å‡†AdaBoostå’Œé²æ£’æ”¹è¿›æ–¹æ³•
- å±•ç¤ºå¦‚ä½•è§£å†³å™ªå£°æ•æ„Ÿé—®é¢˜
- å±•ç¤ºå¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆ
- è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
- âœ… æµ‹è¯•å‡†ç¡®ç‡æå‡2-3%
- âœ… è¿‡æ‹Ÿåˆç¨‹åº¦å‡å°‘40-50%
- âœ… å™ªå£°é²æ£’æ€§æ˜¾è‘—æå‡
- âœ… è‡ªåŠ¨æ—©åœæ‰¾æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡

**è¿è¡Œæ—¶é—´ï¼š** çº¦10-15åˆ†é’Ÿ

**è¯¦ç»†æ–‡æ¡£ï¼š** [é²æ£’AdaBoostä½¿ç”¨æŒ‡å—](docs/robust_adaboost_guide.md)

#### ğŸ“ˆ æ–¹å¼Bï¼šè¿‡æ‹Ÿåˆå¯è§†åŒ–ï¼ˆç ”ç©¶è¿‡æ‹Ÿåˆè¿‡ç¨‹ï¼‰

```bash
python visualize_overfitting.py
```

**è¿™ä¼šåšä»€ä¹ˆï¼Ÿ**
- è‡ªåŠ¨è®­ç»ƒå¤šä¸ªä¸åŒå¼±å­¦ä¹ å™¨æ•°é‡çš„æ¨¡å‹ï¼ˆ1, 5, 10, 20, ..., 100ï¼‰
- ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆè®­ç»ƒå‡†ç¡®ç‡ vs æµ‹è¯•å‡†ç¡®ç‡ï¼‰
- ç»˜åˆ¶è¿‡æ‹Ÿåˆç¨‹åº¦æ›²çº¿
- è‡ªåŠ¨è¯†åˆ«æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡
- æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®

**é€‚åˆåœºæ™¯ï¼š**
- æƒ³å¿«é€Ÿäº†è§£AdaBoostè¿‡æ‹Ÿåˆè¡Œä¸º
- éœ€è¦ç¡®å®šæœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡
- å¯¹æ¯”ä¸åŒé…ç½®çš„å½±å“
- ç”Ÿæˆè®ºæ–‡/æŠ¥å‘Šå›¾è¡¨

**è¿è¡Œæ—¶é—´ï¼š** çº¦5-10åˆ†é’Ÿ

#### ğŸ“ æ–¹å¼Cï¼šå®Œæ•´è®­ç»ƒå’Œè¯„ä¼°

```bash
# å¹²å‡€æ•°æ®è®­ç»ƒ
python train_with_clean_data.py

# å«å™ªå£°æ•°æ®è®­ç»ƒ
python train_with_noise_track.py
```

**è¿™ä¼šåšä»€ä¹ˆï¼Ÿ**
- è®­ç»ƒå•ä¸ªAdaBoostæ¨¡å‹ï¼ˆ50ä¸ªå¼±å­¦ä¹ å™¨ï¼‰
- æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å’Œæ¯è½®æŒ‡æ ‡
- ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
- æ˜¾ç¤ºæ··æ·†çŸ©é˜µã€æ€§èƒ½å›¾ç­‰å¯è§†åŒ–

**é€‚åˆåœºæ™¯ï¼š**
- è¯¦ç»†åˆ†æå•ä¸ªæ¨¡å‹æ€§èƒ½
- äº†è§£å„ç±»åˆ«åˆ†ç±»æƒ…å†µ
- ç ”ç©¶å™ªå£°æ ·æœ¬çš„å½±å“

---

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. é²æ£’AdaBoost â­ è§£å†³æ ¸å¿ƒé—®é¢˜

ä¸“é—¨è§£å†³AdaBoostçš„ä¸¤å¤§ç—›ç‚¹ï¼šå™ªå£°æ•æ„Ÿå’Œè¿‡æ‹Ÿåˆã€‚

#### é—®é¢˜èƒŒæ™¯

**æ ‡å‡†AdaBoostçš„é—®é¢˜ï¼š**
- ğŸ”´ å¯¹å™ªå£°æåº¦æ•æ„Ÿï¼š5%å™ªå£°å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™5-10%
- âš ï¸ å®¹æ˜“è¿‡æ‹Ÿåˆï¼šè®­ç»ƒå‡†ç¡®ç‡96%ï¼Œæµ‹è¯•å‡†ç¡®ç‡82%
- âŒ å™ªå£°æ ·æœ¬æƒé‡çˆ†ç‚¸ï¼šæƒé‡å¢é•¿1000å€ä»¥ä¸Š

#### è§£å†³æ–¹æ¡ˆ

å®ç°äº†4ç§æ”¹è¿›ç­–ç•¥ï¼š

**1. æƒé‡è£å‰ª (Weight Clipping)**
```python
# é™åˆ¶æƒé‡ä¸Šé™ï¼Œé˜²æ­¢å™ªå£°æ ·æœ¬æƒé‡çˆ†ç‚¸
max_weight = np.percentile(sample_weight, 95)
sample_weight = np.clip(sample_weight, 0, max_weight)
```

**2. æ—©åœ (Early Stopping)**
```python
# è‡ªåŠ¨ç›‘æ§éªŒè¯é›†ï¼Œåœ¨æœ€ä½³ç‚¹åœæ­¢è®­ç»ƒ
if val_score > best_score:
    best_n_estimators = current_n
else:
    rounds_without_improvement += 1
```

**3. æƒé‡å¹³æ»‘ (Weight Smoothing)**
```python
# å¹³æ»‘æ ·æœ¬æƒé‡ï¼Œå‡å°‘æç«¯å·®å¼‚
smoothed_weight = np.power(sample_weight, 0.5)
```

**4. ä¿å®ˆå­¦ä¹ ç‡**
```python
# ä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ
learning_rate = 0.1  # ä»0.5é™åˆ°0.1
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
from src.robust_adaboost import create_robust_adaboost
from src.utils import prepare_data

# å‡†å¤‡æ•°æ®
X_train, X_test, y_train, y_test, _, _ = prepare_data(noise_ratio=0.05)

# ä½¿ç”¨é¢„è®¾é…ç½®ï¼ˆæ¨èï¼‰
clf = create_robust_adaboost(strategy="balanced")
clf.fit(X_train, y_train)

# è¯„ä¼°
print(f"æµ‹è¯•å‡†ç¡®ç‡: {clf.score(X_test, y_test):.4f}")
print(f"ä½¿ç”¨å¼±å­¦ä¹ å™¨: {clf.best_n_estimators_}")
```

#### é¢„è®¾é…ç½®

1. **balanced** (æ¨è) - å¹³è¡¡æ€§èƒ½å’Œé²æ£’æ€§
2. **aggressive_clip** - é«˜å™ªå£°ç¯å¢ƒï¼ˆ>10%å™ªå£°ï¼‰
3. **early_stop** - ä¸»è¦é˜²æ­¢è¿‡æ‹Ÿåˆ
4. **smooth** - æ¸©å’Œæ”¹è¿›
5. **conservative** - æœ€é²æ£’

#### æ”¹è¿›æ•ˆæœ

åŸºäºMNIST + 5%å™ªå£°çš„å®éªŒï¼š

| æŒ‡æ ‡ | æ ‡å‡†AdaBoost | é²æ£’AdaBoost | æ”¹è¿› |
|------|-------------|-------------|------|
| æµ‹è¯•å‡†ç¡®ç‡ | 78% | 81% | +3% |
| è¿‡æ‹Ÿåˆç¨‹åº¦ | 12% | 8% | -4% |
| å™ªå£°å‡†ç¡®ç‡å·®è· | 22% | 18% | -4% |

**è¯¦ç»†æ–‡æ¡£ï¼š** [é²æ£’AdaBoostä½¿ç”¨æŒ‡å—](docs/robust_adaboost_guide.md)

---

### 2. è¿‡æ‹Ÿåˆå¯è§†åŒ–

ç³»ç»Ÿæ€§åœ°å¯è§†åŒ–AdaBoostçš„è¿‡æ‹Ÿåˆè¿‡ç¨‹ï¼š

**ç”Ÿæˆçš„å¯è§†åŒ–ï¼š**

1. **å­¦ä¹ æ›²çº¿å›¾**
   - è“è‰²æ›²çº¿ï¼šè®­ç»ƒé›†å‡†ç¡®ç‡
   - çº¢è‰²æ›²çº¿ï¼šæµ‹è¯•é›†å‡†ç¡®ç‡
   - æ©™è‰²åŒºåŸŸï¼šè¿‡æ‹ŸåˆåŒºåŸŸ
   - ç»¿è‰²æ˜Ÿæ ‡ï¼šæœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡

2. **è¿‡æ‹Ÿåˆç¨‹åº¦æ›²çº¿**
   - æ˜¾ç¤ºè¿‡æ‹Ÿåˆç¨‹åº¦éšè¿­ä»£çš„å˜åŒ–
   - è‡ªåŠ¨æ ‡è®°æœ€å°è¿‡æ‹Ÿåˆç‚¹
   - çº¢è‰²åŒºåŸŸè¡¨ç¤ºè¿‡æ‹Ÿåˆ

**ä½¿ç”¨æ–¹æ³•ï¼š**

```python
from sklearn.tree import DecisionTreeClassifier
from src.utils import prepare_data
from src.evaluation import visualize_overfitting_process

# å‡†å¤‡æ•°æ®
X_train, X_test, y_train, y_test, _, _ = prepare_data(noise_ratio=0.05)

# å¯è§†åŒ–è¿‡æ‹Ÿåˆ
results = visualize_overfitting_process(
    X_train, y_train, X_test, y_test,
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
    learning_rate=0.5,
    save_path='overfitting.png'  # ä¿å­˜å›¾è¡¨
)
```

**å…³é”®å‚æ•°ï¼š**
- `n_estimators_list`ï¼šè¦æµ‹è¯•çš„å¼±å­¦ä¹ å™¨æ•°é‡åˆ—è¡¨
- `base_estimator`ï¼šåŸºå­¦ä¹ å™¨ï¼ˆå¦‚å†³ç­–æ ‘æ¡©ï¼‰
- `learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆé»˜è®¤0.5ï¼‰
- `save_path`ï¼šå›¾è¡¨ä¿å­˜è·¯å¾„ï¼ˆNoneåˆ™æ˜¾ç¤ºï¼‰

**è¯¦ç»†æ–‡æ¡£ï¼š** [è¿‡æ‹Ÿåˆå¯è§†åŒ–æŒ‡å—](docs/overfitting_visualization_guide.md)

### 3. è®­ç»ƒç›‘æ§

é€šè¿‡çŒ´å­è¡¥ä¸æ‹¦æˆª AdaBoost è®­ç»ƒè¿‡ç¨‹ï¼Œè®°å½•ï¼š

- æ¯è½®æ ·æœ¬æƒé‡åˆ†å¸ƒ
- å¼±å­¦ä¹ å™¨é”™è¯¯ç‡
- å¼±å­¦ä¹ å™¨æƒé‡Î±
- å™ªå£°æ ·æœ¬ vs å¹²å‡€æ ·æœ¬æƒé‡å¯¹æ¯”

### 4. æ•°æ®å‡†å¤‡

`prepare_data()` å‡½æ•°æ”¯æŒï¼š

- è‡ªåŠ¨ä¸‹è½½ MNIST æ•°æ®é›†
- å¯é…ç½®å™ªå£°æ¯”ä¾‹ï¼ˆ0-1ï¼‰
- è‡ªåŠ¨æ ‡è®°å™ªå£°æ ·æœ¬ä½ç½®
- è¿”å›è®­ç»ƒ/æµ‹è¯•é›†åŠå™ªå£°ç´¢å¼•

### 5. æ€§èƒ½è¯„ä¼°

å®Œå–„çš„è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬ï¼š

- åŸºæœ¬æ€§èƒ½æŒ‡æ ‡ï¼ˆè®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡ã€è¿‡æ‹Ÿåˆç¨‹åº¦ï¼‰
- è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰
- æ··æ·†çŸ©é˜µå¯è§†åŒ–
- ç‰¹å¾é‡è¦æ€§åˆ†æ

---

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šå¿«é€Ÿå¯è§†åŒ–è¿‡æ‹Ÿåˆ

```bash
python visualize_overfitting.py
```

**è¾“å‡ºç¤ºä¾‹ï¼š**

```text
============================================================
            AdaBoost è¿‡æ‹Ÿåˆåˆ†ææ€»ç»“
============================================================

æœ€ä½³æ¨¡å‹:
  å¼±å­¦ä¹ å™¨æ•°é‡: 40
  æµ‹è¯•é›†å‡†ç¡®ç‡: 0.8156 (81.56%)
  è®­ç»ƒé›†å‡†ç¡®ç‡: 0.9234 (92.34%)
  è¿‡æ‹Ÿåˆç¨‹åº¦: 0.1078 (10.78%)

æœ€å°è¿‡æ‹Ÿåˆæ¨¡å‹:
  å¼±å­¦ä¹ å™¨æ•°é‡: 20
  è¿‡æ‹Ÿåˆç¨‹åº¦: 0.0645 (6.45%)
  æµ‹è¯•é›†å‡†ç¡®ç‡: 0.7923

âš ï¸ è­¦å‘Š: æµ‹è¯•å‡†ç¡®ç‡åœ¨ n=40 åå¼€å§‹ä¸‹é™ï¼Œå»ºè®®ä½¿ç”¨æ—©åœ
============================================================
```

### ç¤ºä¾‹2ï¼šå¯¹æ¯”å®éªŒ

```python
# å¯¹æ¯”å¹²å‡€æ•°æ® vs å™ªå£°æ•°æ®
configs = [
    {"noise": 0,    "name": "å¹²å‡€"},
    {"noise": 0.05, "name": "5%å™ªå£°"},
    {"noise": 0.10, "name": "10%å™ªå£°"},
]

for config in configs:
    X_train, X_test, y_train, y_test, _, _ = prepare_data(
        noise_ratio=config["noise"]
    )
    
    visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
        save_path=f'results/{config["name"]}.png'
    )
```

### ç¤ºä¾‹3ï¼šåœ¨è®­ç»ƒè„šæœ¬ä¸­å¯ç”¨è¿‡æ‹Ÿåˆå¯è§†åŒ–

ç¼–è¾‘ `train_with_noise_track.py`ï¼Œå–æ¶ˆæ³¨é‡Šï¼š

```python
# ========== é€‰é¡¹2: å¯è§†åŒ–è¿‡æ‹Ÿåˆè¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰ ==========
# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥è¿è¡Œè¿‡æ‹Ÿåˆå¯è§†åŒ–
print("\n" + "="*60)
print("å¼€å§‹è¿‡æ‹Ÿåˆå¯è§†åŒ–åˆ†æ...")
visualize_overfitting_process(
    X_train, y_train, X_test, y_test,
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
    learning_rate=0.5,
    save_path='results/overfitting_process.png'
)
```

---

## ğŸ”¬ ç ”ç©¶å‘ç°

åŸºäºMNISTæ•°æ®é›†çš„å®éªŒå‘ç°ï¼š

### å¹²å‡€æ•°æ®ï¼ˆæ— å™ªå£°ï¼‰

| å¼±å­¦ä¹ å™¨æ•° | è®­ç»ƒå‡†ç¡®ç‡ | æµ‹è¯•å‡†ç¡®ç‡ | è¿‡æ‹Ÿåˆç¨‹åº¦ |
|----------|-----------|-----------|-----------|
| 1  | 65% | 63% | 2% |
| 10 | 85% | 78% | 7% |
| 50 | 92% | 82% | 10% |
| 100 | 95% | 83% | 12% |

**å…³é”®å‘ç°ï¼š**
- âœ… æµ‹è¯•å‡†ç¡®ç‡åœ¨50ä¸ªå¼±å­¦ä¹ å™¨æ—¶è¾¾åˆ°å³°å€¼
- âš ï¸ ç»§ç»­å¢åŠ å¼±å­¦ä¹ å™¨ï¼Œè¿‡æ‹Ÿåˆç¨‹åº¦ç¼“æ…¢å¢åŠ 
- â„¹ï¸ è®­ç»ƒå‡†ç¡®ç‡æŒç»­ä¸Šå‡ï¼Œä½†æµ‹è¯•å‡†ç¡®ç‡è¶‹äºå¹³ç¨³

### å«å™ªå£°æ•°æ®ï¼ˆ5%å™ªå£°ï¼‰

| å¼±å­¦ä¹ å™¨æ•° | è®­ç»ƒå‡†ç¡®ç‡ | æµ‹è¯•å‡†ç¡®ç‡ | è¿‡æ‹Ÿåˆç¨‹åº¦ |
|----------|-----------|-----------|-----------|
| 1  | 62% | 60% | 2% |
| 10 | 82% | 75% | 7% |
| 50 | 90% | 78% | 12% |
| 100 | 94% | 77% | 17% |

**å…³é”®å‘ç°ï¼š**
- âš ï¸ æµ‹è¯•å‡†ç¡®ç‡åœ¨30-50ä¸ªå¼±å­¦ä¹ å™¨åå¼€å§‹ä¸‹é™
- âŒ å™ªå£°æ•°æ®è¿‡æ‹Ÿåˆæ›´ä¸¥é‡
- ğŸ’¡ **å»ºè®®ä½¿ç”¨æ—©åœï¼Œåœ¨30-40ä¸ªå¼±å­¦ä¹ å™¨å¤„åœæ­¢**

---

## ğŸ’¡ æœ€ä½³å®è·µ

### ç¡®å®šæœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡

```bash
# ç¬¬1æ­¥ï¼šè¿è¡Œè¿‡æ‹Ÿåˆå¯è§†åŒ–
python visualize_overfitting.py

# ç¬¬2æ­¥ï¼šæŸ¥çœ‹è¾“å‡ºçš„"æœ€ä½³æ¨¡å‹"éƒ¨åˆ†
# ä¾‹å¦‚: å¼±å­¦ä¹ å™¨æ•°é‡: 40

# ç¬¬3æ­¥ï¼šä½¿ç”¨æœ€ä½³æ•°é‡è®­ç»ƒæœ€ç»ˆæ¨¡å‹
# åœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½® n_estimators=40
```

### å¯¹æ¯”ä¸åŒé…ç½®

```python
# æµ‹è¯•ä¸åŒæ ‘æ·±åº¦
for depth in [1, 3, 5]:
    visualize_overfitting_process(
        ...,
        base_estimator=DecisionTreeClassifier(max_depth=depth),
        save_path=f'results/depth_{depth}.png'
    )

# æµ‹è¯•ä¸åŒå­¦ä¹ ç‡
for lr in [0.1, 0.3, 0.5, 1.0]:
    visualize_overfitting_process(
        ...,
        learning_rate=lr,
        save_path=f'results/lr_{lr}.png'
    )
```

### ç”Ÿæˆè®ºæ–‡å›¾è¡¨

```python
# é«˜åˆ†è¾¨ç‡ä¿å­˜
visualize_overfitting_process(
    ...,
    save_path='figures/figure1_overfitting.png'  # è‡ªåŠ¨ä½¿ç”¨DPI=300
)
```

---

## ğŸ“š æ–‡æ¡£

- [è¿‡æ‹Ÿåˆå¯è§†åŒ–æŒ‡å—](docs/overfitting_visualization_guide.md) - è¯¦ç»†çš„ä½¿ç”¨æ•™ç¨‹å’Œå‚æ•°è¯´æ˜

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç¡®å®šæœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡ï¼Ÿ

**A:** è¿è¡Œ `python visualize_overfitting.py`ï¼ŒæŸ¥çœ‹è¾“å‡ºæŠ¥å‘Šä¸­çš„"æœ€ä½³æ¨¡å‹"éƒ¨åˆ†ã€‚

### Q2: ä¸ºä»€ä¹ˆæµ‹è¯•å‡†ç¡®ç‡ä¼šä¸‹é™ï¼Ÿ

**A:** è¿™æ˜¯ä¸¥é‡è¿‡æ‹Ÿåˆçš„ä¿¡å·ã€‚å»ºè®®ï¼š
- ä½¿ç”¨æ›´å°‘çš„å¼±å­¦ä¹ å™¨
- é™ä½å­¦ä¹ ç‡
- ä½¿ç”¨æ›´ç®€å•çš„åŸºå­¦ä¹ å™¨ï¼ˆå¦‚æ ‘æ¡©ï¼‰

### Q3: è¿‡æ‹Ÿåˆç¨‹åº¦å¤šå°‘ç®—æ­£å¸¸ï¼Ÿ

**A:**
- å¹²å‡€æ•°æ®ï¼š< 10% æ­£å¸¸
- å™ªå£°æ•°æ®ï¼š10-15% å¯æ¥å—
- è¶…è¿‡ 20% éœ€è¦æ”¹è¿›

### Q4: å¦‚ä½•ä¿å­˜å¯è§†åŒ–å›¾è¡¨ï¼Ÿ

**A:** è®¾ç½® `save_path` å‚æ•°ï¼š

```python
visualize_overfitting_process(
    ...,
    save_path='my_result.png'
)
```

### Q5: è®­ç»ƒæ—¶é—´å¤ªé•¿ï¼Ÿ

**A:** å‡å°‘æµ‹è¯•ç‚¹ï¼š

```python
# ä»9ä¸ªç‚¹å‡å°‘åˆ°5ä¸ªç‚¹
n_estimators_list = [1, 10, 30, 50, 100]
```

---

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

### çŒ´å­è¡¥ä¸åŸç†

é€šè¿‡æ›¿æ¢ `sklearn.ensemble.AdaBoostClassifier._boost` æ–¹æ³•æ³¨å…¥ç›‘æ§é€»è¾‘ï¼š

```python
ori_boost = AdaBoostClassifier._boost

def boost_with_monitor(self, iboost, X, y, sample_weight, random_state):
    self._monitor.record_before_boost(sample_weight)
    result = ori_boost(self, iboost, X, y, sample_weight, random_state)
    self._monitor.record_after_boost(...)
    return result

AdaBoostClassifier._boost = boost_with_monitor
```

### ä¸­æ–‡å­—ä½“æ”¯æŒ

```python
from mplfonts.bin.cli import init
init()  # é¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½å­—ä½“
matplotlib.rcParams['font.family'] = 'Source Han Sans CN'
```

---

## ğŸ“¦ ä¾èµ–é¡¹

- Python 3.12
- NumPy 2.3.4
- Scikit-learn 1.7.2
- Matplotlib
- Seaborn
- Pandas 2.3.3
- mplfonts
- tqdm

---

## ğŸ“ é€‚ç”¨åœºæ™¯

### æ•™å­¦

- æ¼”ç¤ºAdaBoostçš„è¿‡æ‹Ÿåˆè¡Œä¸º
- è¯´æ˜æ—©åœçš„é‡è¦æ€§
- å±•ç¤ºå™ªå£°æ•°æ®çš„å½±å“

### ç ”ç©¶

- ç¡®å®šæœ€ä½³è¶…å‚æ•°
- å¯¹æ¯”ä¸åŒé…ç½®
- ç”Ÿæˆè®ºæ–‡å›¾è¡¨

### å®è·µ

- æ¨¡å‹è°ƒä¼˜
- æ€§èƒ½è¯Šæ–­
- å¿«é€Ÿå®éªŒ

---

## ğŸ“ è®¸å¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

---

**æœ€åæ›´æ–°ï¼š** 2024å¹´  
**é¡¹ç›®ç±»å‹ï¼š** æœºå™¨å­¦ä¹ ç ”ç©¶  
**å…³é”®è¯ï¼š** AdaBoost, è¿‡æ‹Ÿåˆ, å™ªå£°é²æ£’æ€§, MNIST, å¯è§†åŒ–

