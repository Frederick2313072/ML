# æ¨¡å‹æ‹Ÿåˆè¿‡ç¨‹å¯è§†åŒ–å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨é¡¹ç›®ä¸­å¯è§†åŒ– AdaBoost æ¨¡å‹çš„æ‹Ÿåˆè¿‡ç¨‹ã€‚

## ğŸ“Š å¯è§†åŒ–æ–¹æ³•æ€»è§ˆ

é¡¹ç›®æä¾›ä¸‰ç§ä¸»è¦çš„å¯è§†åŒ–æ–¹å¼ï¼š

| æ–¹æ³• | æ–‡ä»¶ | ä¸»è¦åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ |
|-----|-----|---------|---------|
| è¿‡æ‹Ÿåˆå¯è§†åŒ– | `visualize_overfitting.py` | å­¦ä¹ æ›²çº¿ã€è¿‡æ‹Ÿåˆåˆ†æ | å¿«é€Ÿè¯Šæ–­ã€å‚æ•°é€‰æ‹© |
| è®­ç»ƒç›‘æ§ | `train_with_noise_track.py` | å®æ—¶è¿½è¸ªè®­ç»ƒè¿‡ç¨‹ | æ·±å…¥åˆ†æã€æ ·æœ¬æƒé‡ç ”ç©¶ |
| é²æ£’æ–¹æ³•å¯¹æ¯” | `compare_robust_methods.py` | å¤šæ–¹æ³•æ€§èƒ½å¯¹æ¯” | æ–¹æ³•é€‰æ‹©ã€æ•ˆæœéªŒè¯ |

---

## ğŸŒŸ æ–¹æ³•1ï¼šè¿‡æ‹Ÿåˆå¯è§†åŒ–ï¼ˆæ¨èï¼‰

### å¿«é€Ÿå¼€å§‹

```bash
python visualize_overfitting.py
```

### è¯¦ç»†ç”¨æ³•

```python
from sklearn.tree import DecisionTreeClassifier
from src.utils import prepare_data
from src.evaluation import visualize_overfitting_process

# 1. å‡†å¤‡æ•°æ®
X_train, X_test, y_train, y_test, _, _ = prepare_data(noise_ratio=0.05)

# 2. è¿è¡Œå¯è§†åŒ–
results = visualize_overfitting_process(
    X_train, y_train, X_test, y_test,
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
    learning_rate=0.5,
    random_state=42,
    save_path='overfitting_analysis.png'  # Noneåˆ™ç›´æ¥æ˜¾ç¤º
)
```

### å‚æ•°è¯´æ˜

#### `n_estimators_list` - æµ‹è¯•ç‚¹é…ç½®

```python
# é…ç½®1: å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èï¼‰
n_estimators_list=[1, 10, 30, 50, 100]  # 5ä¸ªç‚¹ï¼Œçº¦3åˆ†é’Ÿ

# é…ç½®2: æ ‡å‡†æµ‹è¯•ï¼ˆé»˜è®¤ï¼‰
n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100]  # 9ä¸ªç‚¹ï¼Œçº¦5-10åˆ†é’Ÿ

# é…ç½®3: ç²¾ç»†åˆ†æ
n_estimators_list=list(range(1, 51, 2))  # 25ä¸ªç‚¹ï¼Œçº¦15-20åˆ†é’Ÿ

# é…ç½®4: æ‰©å±•èŒƒå›´
n_estimators_list=[1, 10, 20, 50, 100, 150, 200]  # æµ‹è¯•æ›´å¤šå¼±å­¦ä¹ å™¨
```

#### `base_estimator` - åŸºå­¦ä¹ å™¨é…ç½®

```python
# å†³ç­–æ ‘æ¡©ï¼ˆæœ€å¸¸ç”¨ï¼‰
base_estimator=DecisionTreeClassifier(max_depth=1)

# æ·±åº¦3çš„æ ‘ï¼ˆæ›´å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
base_estimator=DecisionTreeClassifier(max_depth=3)

# æ·±åº¦5çš„æ ‘ï¼ˆè§‚å¯Ÿä¸¥é‡è¿‡æ‹Ÿåˆï¼‰
base_estimator=DecisionTreeClassifier(max_depth=5)
```

#### `learning_rate` - å­¦ä¹ ç‡

```python
# é«˜å­¦ä¹ ç‡ï¼ˆæ”¶æ•›å¿«ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
learning_rate=1.0

# æ ‡å‡†å­¦ä¹ ç‡ï¼ˆæ¨èï¼‰
learning_rate=0.5

# ä½å­¦ä¹ ç‡ï¼ˆæ”¶æ•›æ…¢ï¼Œæ³›åŒ–å¥½ï¼‰
learning_rate=0.1
```

### è¾“å‡ºè§£è¯»

#### å›¾è¡¨1ï¼šå­¦ä¹ æ›²çº¿

```
å‡†ç¡®ç‡
  â”‚
1.0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  è“è‰²ï¼šè®­ç»ƒå‡†ç¡®ç‡ï¼ˆæŒç»­ä¸Šå‡ï¼‰
  â”‚            â•±
0.9â”‚          â•±  â”€â”€â”€â”€ çº¢è‰²ï¼šæµ‹è¯•å‡†ç¡®ç‡ï¼ˆå¯èƒ½å¹³ç¨³æˆ–ä¸‹é™ï¼‰
  â”‚        â•±   â•±
0.8â”‚      â•±  â•±    â˜…   ç»¿è‰²æ˜Ÿæ ‡ï¼šæœ€ä½³æ¨¡å‹ç‚¹
  â”‚    â•±  â•±     
0.7â”‚  â•± â•±          ğŸŸ  æ©™è‰²åŒºåŸŸï¼šè¿‡æ‹Ÿåˆå·®è·
  â”‚â•± â•±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> å¼±å­¦ä¹ å™¨æ•°é‡
```

**å…³é”®ä¿¡æ¯ï¼š**
- ä¸¤æ›²çº¿å·®è· = è¿‡æ‹Ÿåˆç¨‹åº¦
- æµ‹è¯•å‡†ç¡®ç‡å³°å€¼ = æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡
- æµ‹è¯•å‡†ç¡®ç‡ä¸‹é™ = ä¸¥é‡è¿‡æ‹Ÿåˆè­¦å‘Š

#### å›¾è¡¨2ï¼šè¿‡æ‹Ÿåˆç¨‹åº¦æ›²çº¿

```
è¿‡æ‹Ÿåˆåº¦
  â”‚
0.2â”œâ”€     â•±â”€â”€â•²
  â”‚    â•±      â•²     ğŸŸ¥ çº¢è‰²ï¼šè¿‡æ‹ŸåˆåŒºåŸŸ
0.1â”œâ”€ â•±    â˜…   â”€â”€â•²  â˜… æœ€å°è¿‡æ‹Ÿåˆç‚¹
  â”‚ â•±
0.0â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  é»‘è‰²è™šçº¿ï¼šå®Œç¾æ‹Ÿåˆ
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> å¼±å­¦ä¹ å™¨æ•°é‡
```

**è¯„ä»·æ ‡å‡†ï¼š**

| è¿‡æ‹Ÿåˆç¨‹åº¦ | è¯„ä»· | è¯´æ˜ |
|----------|------|------|
| < 0.05 | âœ… ä¼˜ç§€ | æ¨¡å‹æ³›åŒ–è‰¯å¥½ |
| 0.05-0.10 | âœ… è‰¯å¥½ | è½»å¾®è¿‡æ‹Ÿåˆï¼Œå¯æ¥å— |
| 0.10-0.15 | âš ï¸ ä¸€èˆ¬ | ä¸­åº¦è¿‡æ‹Ÿåˆ |
| 0.15-0.20 | âš ï¸ è¾ƒå·® | æ˜æ˜¾è¿‡æ‹Ÿåˆ |
| > 0.20 | âŒ å·® | ä¸¥é‡è¿‡æ‹Ÿåˆ |

#### æ–‡æœ¬æŠ¥å‘Š

```text
============================================================
            AdaBoost è¿‡æ‹Ÿåˆåˆ†ææ€»ç»“
============================================================

æœ€ä½³æ¨¡å‹:
  å¼±å­¦ä¹ å™¨æ•°é‡: 40
  æµ‹è¯•é›†å‡†ç¡®ç‡: 0.8156 (81.56%)
  è®­ç»ƒé›†å‡†ç¡®ç‡: 0.9234 (92.34%)
  è¿‡æ‹Ÿåˆç¨‹åº¦: 0.1078 (10.78%)

æœ€å°è¿‡æ‹Ÿåˆæ¨¡å‹:
  å¼±å­¦ä¹ å™¨æ•°é‡: 20
  è¿‡æ‹Ÿåˆç¨‹åº¦: 0.0645 (6.45%)
  æµ‹è¯•é›†å‡†ç¡®ç‡: 0.7923

è¶‹åŠ¿åˆ†æ:
  åˆå§‹ (n=1): æµ‹è¯•å‡†ç¡®ç‡ = 0.6234, è¿‡æ‹Ÿåˆ = 0.0156
  æœ€ç»ˆ (n=100): æµ‹è¯•å‡†ç¡®ç‡ = 0.8034, è¿‡æ‹Ÿåˆ = 0.1534
  âš ï¸ è­¦å‘Š: æµ‹è¯•å‡†ç¡®ç‡åœ¨ n=40 åå¼€å§‹ä¸‹é™ï¼Œå»ºè®®ä½¿ç”¨æ—©åœ
============================================================
```

---

## ğŸ“ æ–¹æ³•2ï¼šè®­ç»ƒç›‘æ§

### å¿«é€Ÿå¼€å§‹

```bash
# å™ªå£°æ•°æ®è®­ç»ƒï¼ˆæ¨èï¼Œæ›´èƒ½å±•ç¤ºé—®é¢˜ï¼‰
python train_with_noise_track.py

# å¹²å‡€æ•°æ®è®­ç»ƒ
python train_with_clean_data.py
```

### å®æ—¶è¾“å‡ºç¤ºä¾‹

```text
[BOOST] 5/50 | error=0.0234 | alpha=1.2345 | noisy_w=0.023456
[VAL]   round=005 | acc=  0.7234 | f1=  0.7156
[TRAIN] round=005 | acc=  0.8456 | f1=  0.8389

[BOOST] 10/50 | error=0.0189 | alpha=1.3456 | noisy_w=0.034567
[VAL]   round=010 | acc=  0.7456 | f1=  0.7389
[TRAIN] round=010 | acc=  0.8789 | f1=  0.8712

[CHECKPOINT] Saved 'experiments/my_exp/checkpoints/round_0050.csv' (round=50)
```

### æ£€æŸ¥ç‚¹æ•°æ®

æ¯éš”50è½®è‡ªåŠ¨ä¿å­˜CSVæ–‡ä»¶ï¼ŒåŒ…å«ï¼š

```csv
round,weighted_error,alpha,acc_on_training_data,val_acc_history,noisy_weight,clean_weight
1,0.0245,1.2134,0.7234,0.6789,0.0123,0.9877
2,0.0234,1.2345,0.7456,0.7012,0.0156,0.9844
...
```

### ä½¿ç”¨ç›‘æ§æ•°æ®è¿›è¡Œè‡ªå®šä¹‰å¯è§†åŒ–

```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ£€æŸ¥ç‚¹æ•°æ®
df = pd.read_csv('experiments/my_exp/results/final_results.csv')

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.figure(figsize=(12, 5))

# å­å›¾1ï¼šå‡†ç¡®ç‡æ¼”åŒ–
plt.subplot(1, 2, 1)
plt.plot(df['round'], df['acc_on_training_data'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(df['round'], df['val_acc_history'], label='éªŒè¯å‡†ç¡®ç‡')
plt.xlabel('è®­ç»ƒè½®æ¬¡')
plt.ylabel('å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)

# å­å›¾2ï¼šæ ·æœ¬æƒé‡æ¼”åŒ–
plt.subplot(1, 2, 2)
plt.plot(df['round'], df['noisy_weight'], label='å™ªå£°æ ·æœ¬æƒé‡', color='red')
plt.plot(df['round'], df['clean_weight'], label='å¹²å‡€æ ·æœ¬æƒé‡', color='green')
plt.xlabel('è®­ç»ƒè½®æ¬¡')
plt.ylabel('æƒé‡æ€»å’Œ')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('custom_monitoring.png', dpi=300)
plt.show()
```

---

## ğŸ›¡ï¸ æ–¹æ³•3ï¼šé²æ£’æ–¹æ³•å¯¹æ¯”

### å¿«é€Ÿæ¼”ç¤º

```bash
python demo_robust.py
```

**è¾“å‡ºï¼š**
- æ ‡å‡†AdaBoost vs é²æ£’æ”¹è¿›æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
- è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”
- å»ºè®®ä½¿ç”¨çš„æ–¹æ³•

### å®Œæ•´å®éªŒå¯¹æ¯”

```bash
python compare_robust_methods.py
```

**æµ‹è¯•çš„æ–¹æ³•ï¼š**
1. æ ‡å‡†AdaBoostï¼ˆbaselineï¼‰
2. æƒé‡è£å‰ªï¼ˆweight_clippingï¼‰
3. æ—©åœï¼ˆearly_stoppingï¼‰
4. æƒé‡å¹³æ»‘ï¼ˆweight_smoothingï¼‰
5. ä¿å®ˆå­¦ä¹ ç‡ï¼ˆconservativeï¼‰
6. å¹³è¡¡é…ç½®ï¼ˆbalancedï¼‰

**è¾“å‡ºç¤ºä¾‹ï¼š**

```text
============================================================
            é²æ£’æ–¹æ³•æ€§èƒ½å¯¹æ¯”
============================================================

æ–¹æ³•æ’åï¼ˆæŒ‰æµ‹è¯•å‡†ç¡®ç‡ï¼‰:
1. balanced:           æµ‹è¯•=0.8234, è¿‡æ‹Ÿåˆ=0.0845
2. early_stopping:     æµ‹è¯•=0.8189, è¿‡æ‹Ÿåˆ=0.0789
3. weight_clipping:    æµ‹è¯•=0.8156, è¿‡æ‹Ÿåˆ=0.0912
4. baseline:           æµ‹è¯•=0.7789, è¿‡æ‹Ÿåˆ=0.1234

æ¨èæ–¹æ³•: balanced
ç†ç”±: æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡ + è¾ƒä½è¿‡æ‹Ÿåˆç¨‹åº¦
============================================================
```

---

## ğŸ”¬ å®éªŒåœºæ™¯

### åœºæ™¯1ï¼šç¡®å®šæœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡

**ç›®æ ‡ï¼š** æ‰¾åˆ°æ€§èƒ½æœ€ä¼˜çš„å¼±å­¦ä¹ å™¨æ•°é‡

```python
# æ­¥éª¤1ï¼šè¿è¡Œè¿‡æ‹Ÿåˆå¯è§†åŒ–
results = visualize_overfitting_process(
    X_train, y_train, X_test, y_test,
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
    learning_rate=0.5
)

# æ­¥éª¤2ï¼šæå–æœ€ä½³é…ç½®
best_idx = results["test_accuracy"].index(max(results["test_accuracy"]))
best_n_estimators = results["n_estimators"][best_idx]
print(f"æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡: {best_n_estimators}")

# æ­¥éª¤3ï¼šä½¿ç”¨æœ€ä½³é…ç½®è®­ç»ƒæœ€ç»ˆæ¨¡å‹
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=best_n_estimators,
    learning_rate=0.5
)
clf.fit(X_train, y_train)
```

### åœºæ™¯2ï¼šå¯¹æ¯”å¹²å‡€æ•°æ® vs å™ªå£°æ•°æ®

**ç›®æ ‡ï¼š** ç ”ç©¶å™ªå£°å¯¹è¿‡æ‹Ÿåˆçš„å½±å“

```python
noise_levels = [0, 0.05, 0.10]

for noise in noise_levels:
    print(f"\næµ‹è¯•å™ªå£°æ°´å¹³: {noise*100}%")
    
    X_train, X_test, y_train, y_test, _, _ = prepare_data(noise_ratio=noise)
    
    results = visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
        learning_rate=0.5,
        save_path=f'results/noise_{int(noise*100)}.png'
    )
```

**é¢„æœŸå‘ç°ï¼š**
- å™ªå£°æ•°æ®æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡æ›´å°‘
- å™ªå£°æ•°æ®è¿‡æ‹Ÿåˆæ›´ä¸¥é‡
- å™ªå£°æ•°æ®æµ‹è¯•å‡†ç¡®ç‡å³°å€¼æ›´ä½

### åœºæ™¯3ï¼šå¯¹æ¯”ä¸åŒæ ‘æ·±åº¦

**ç›®æ ‡ï¼š** ç ”ç©¶åŸºå­¦ä¹ å™¨å¤æ‚åº¦çš„å½±å“

```python
depths = [1, 3, 5]

for depth in depths:
    print(f"\næµ‹è¯•æ ‘æ·±åº¦: {depth}")
    
    results = visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=depth),
        n_estimators_list=[1, 5, 10, 20, 30, 40, 50],
        learning_rate=0.5,
        save_path=f'results/depth_{depth}.png'
    )
```

**é¢„æœŸå‘ç°ï¼š**
- æ·±æ ‘æ”¶æ•›æ›´å¿«ï¼ˆéœ€è¦æ›´å°‘å¼±å­¦ä¹ å™¨ï¼‰
- æ·±æ ‘æ›´å®¹æ˜“è¿‡æ‹Ÿåˆ
- æ ‘æ¡©ï¼ˆdepth=1ï¼‰æ³›åŒ–æœ€å¥½

### åœºæ™¯4ï¼šå¯¹æ¯”ä¸åŒå­¦ä¹ ç‡

**ç›®æ ‡ï¼š** æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡

```python
learning_rates = [0.1, 0.3, 0.5, 1.0]

for lr in learning_rates:
    print(f"\næµ‹è¯•å­¦ä¹ ç‡: {lr}")
    
    results = visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators_list=[1, 10, 20, 30, 50, 75, 100, 150, 200],
        learning_rate=lr,
        save_path=f'results/lr_{lr}.png'
    )
```

**é¢„æœŸå‘ç°ï¼š**
- ä½å­¦ä¹ ç‡éœ€è¦æ›´å¤šå¼±å­¦ä¹ å™¨
- é«˜å­¦ä¹ ç‡æ”¶æ•›å¿«ä½†å®¹æ˜“è¿‡æ‹Ÿåˆ
- 0.3-0.5é€šå¸¸æ˜¯å¹³è¡¡ç‚¹

---

## ğŸ“ˆ é«˜çº§å¯è§†åŒ–æŠ€å·§

### æŠ€å·§1ï¼šå¯¹æ¯”å¤šä¸ªé…ç½®

```python
import matplotlib.pyplot as plt
import numpy as np

configs = [
    {"depth": 1, "lr": 0.5, "label": "æ ‘æ¡©+æ ‡å‡†LR"},
    {"depth": 3, "lr": 0.5, "label": "æ·±æ ‘+æ ‡å‡†LR"},
    {"depth": 1, "lr": 0.1, "label": "æ ‘æ¡©+ä½LR"},
]

plt.figure(figsize=(12, 5))

# å­å›¾1ï¼šæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”
plt.subplot(1, 2, 1)
for config in configs:
    results = visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=config["depth"]),
        n_estimators_list=[1, 10, 20, 30, 50, 75, 100],
        learning_rate=config["lr"],
        save_path=None  # ä¸ä¿å­˜
    )
    plt.plot(results["n_estimators"], results["test_accuracy"], 
             label=config["label"], marker='o')

plt.xlabel('å¼±å­¦ä¹ å™¨æ•°é‡')
plt.ylabel('æµ‹è¯•å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)
plt.title('æµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”')

# å­å›¾2ï¼šè¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”
plt.subplot(1, 2, 2)
for config in configs:
    results = visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=config["depth"]),
        n_estimators_list=[1, 10, 20, 30, 50, 75, 100],
        learning_rate=config["lr"],
        save_path=None
    )
    plt.plot(results["n_estimators"], results["overfitting_degree"], 
             label=config["label"], marker='s')

plt.xlabel('å¼±å­¦ä¹ å™¨æ•°é‡')
plt.ylabel('è¿‡æ‹Ÿåˆç¨‹åº¦')
plt.legend()
plt.grid(True)
plt.title('è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”')

plt.tight_layout()
plt.savefig('multi_config_comparison.png', dpi=300)
plt.show()
```

### æŠ€å·§2ï¼šåˆ¶ä½œåŠ¨ç”»æ¼”ç¤ºè®­ç»ƒè¿‡ç¨‹

```python
import matplotlib.animation as animation

# è¯»å–æ£€æŸ¥ç‚¹æ•°æ®
df = pd.read_csv('experiments/my_exp/results/final_results.csv')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

def update(frame):
    ax1.clear()
    ax2.clear()
    
    # å­å›¾1ï¼šå‡†ç¡®ç‡æ¼”åŒ–
    ax1.plot(df['round'][:frame], df['acc_on_training_data'][:frame], 
             label='è®­ç»ƒ', color='blue')
    ax1.plot(df['round'][:frame], df['val_acc_history'][:frame], 
             label='éªŒè¯', color='red')
    ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax1.set_ylabel('å‡†ç¡®ç‡')
    ax1.set_ylim(0.6, 1.0)
    ax1.legend()
    ax1.grid(True)
    ax1.set_title(f'å‡†ç¡®ç‡æ¼”åŒ– (è½®æ¬¡: {frame})')
    
    # å­å›¾2ï¼šæ ·æœ¬æƒé‡æ¼”åŒ–
    if 'noisy_weight' in df.columns:
        ax2.plot(df['round'][:frame], df['noisy_weight'][:frame], 
                 label='å™ªå£°æ ·æœ¬', color='red')
        ax2.plot(df['round'][:frame], df['clean_weight'][:frame], 
                 label='å¹²å‡€æ ·æœ¬', color='green')
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('æƒé‡æ€»å’Œ')
        ax2.legend()
        ax2.grid(True)
        ax2.set_title(f'æ ·æœ¬æƒé‡æ¼”åŒ– (è½®æ¬¡: {frame})')

ani = animation.FuncAnimation(fig, update, frames=len(df), interval=100)
ani.save('training_animation.gif', writer='pillow', fps=10)
```

### æŠ€å·§3ï¼šç”Ÿæˆè®ºæ–‡çº§å›¾è¡¨

```python
# è®¾ç½®è®ºæ–‡é£æ ¼
plt.style.use('seaborn-v0_8-paper')
plt.rcParams.update({
    'font.size': 12,
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'font.family': 'serif',
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
})

# ç”Ÿæˆé«˜è´¨é‡å›¾è¡¨
results = visualize_overfitting_process(
    X_train, y_train, X_test, y_test,
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
    learning_rate=0.5,
    save_path='paper_figure1.pdf'  # PDFæ ¼å¼ï¼ŒçŸ¢é‡å›¾
)
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæµ‹è¯•å‡†ç¡®ç‡ä¼šä¸‹é™ï¼Ÿ

**A:** è¿™æ˜¯ä¸¥é‡è¿‡æ‹Ÿåˆçš„ä¿¡å·ã€‚åŸå› ï¼š
- å¼±å­¦ä¹ å™¨æ•°é‡è¿‡å¤š
- å­¦ä¹ ç‡è¿‡é«˜
- åŸºå­¦ä¹ å™¨è¿‡äºå¤æ‚
- æ•°æ®ä¸­æœ‰å™ªå£°

**è§£å†³æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨æ—©åœï¼Œåœ¨å³°å€¼ç‚¹åœæ­¢
2. é™ä½å­¦ä¹ ç‡ï¼ˆ0.5 â†’ 0.1ï¼‰
3. ä½¿ç”¨æ›´ç®€å•çš„åŸºå­¦ä¹ å™¨ï¼ˆæ ‘æ¡©ï¼‰
4. ä½¿ç”¨é²æ£’æ–¹æ³•ï¼ˆè§ `demo_robust.py`ï¼‰

### Q2: å¦‚ä½•ç¡®å®šæ˜¯å¦è¿‡æ‹Ÿåˆï¼Ÿ

**A:** çœ‹ä¸¤ä¸ªæŒ‡æ ‡ï¼š
1. **è¿‡æ‹Ÿåˆç¨‹åº¦** = è®­ç»ƒå‡†ç¡®ç‡ - æµ‹è¯•å‡†ç¡®ç‡
   - < 10%: æ­£å¸¸
   - 10-15%: å¯æ¥å—
   - > 15%: éœ€è¦æ”¹è¿›

2. **æµ‹è¯•å‡†ç¡®ç‡è¶‹åŠ¿**
   - æŒç»­ä¸Šå‡: è‰¯å¥½
   - å¹³ç¨³: å¯æ¥å—
   - ä¸‹é™: âš ï¸ è­¦å‘Š

### Q3: è®­ç»ƒæ—¶é—´å¤ªé•¿æ€ä¹ˆåŠï¼Ÿ

**A:** å‡å°‘æµ‹è¯•ç‚¹ï¼š

```python
# ä»è¿™ä¸ªï¼ˆ9ä¸ªç‚¹ï¼Œ10åˆ†é’Ÿï¼‰
n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100]

# æ”¹ä¸ºè¿™ä¸ªï¼ˆ5ä¸ªç‚¹ï¼Œ5åˆ†é’Ÿï¼‰
n_estimators_list=[1, 10, 30, 50, 100]
```

### Q4: å¦‚ä½•ä¿å­˜å›¾è¡¨ï¼Ÿ

**A:** è®¾ç½® `save_path` å‚æ•°ï¼š

```python
# PNGæ ¼å¼ï¼ˆå±å¹•å±•ç¤ºï¼‰
save_path='my_result.png'

# PDFæ ¼å¼ï¼ˆè®ºæ–‡/æ‰“å°ï¼‰
save_path='my_result.pdf'

# Noneï¼ˆåªæ˜¾ç¤ºä¸ä¿å­˜ï¼‰
save_path=None
```

### Q5: å¦‚ä½•å¯¹æ¯”å¤šä¸ªå®éªŒç»“æœï¼Ÿ

**A:** ä¸‰ç§æ–¹æ³•ï¼š

**æ–¹æ³•1ï¼šä¿å­˜å¤šä¸ªå›¾è¡¨**
```python
for noise in [0, 0.05, 0.10]:
    visualize_overfitting_process(
        ...,
        save_path=f'noise_{int(noise*100)}.png'
    )
```

**æ–¹æ³•2ï¼šä½¿ç”¨ç›‘æ§æ•°æ®**
```python
# è¯»å–å¤šä¸ªå®éªŒçš„CSV
df1 = pd.read_csv('exp1/final_results.csv')
df2 = pd.read_csv('exp2/final_results.csv')

# ç»˜åˆ¶å¯¹æ¯”
plt.plot(df1['round'], df1['val_acc_history'], label='å®éªŒ1')
plt.plot(df2['round'], df2['val_acc_history'], label='å®éªŒ2')
plt.legend()
plt.show()
```

**æ–¹æ³•3ï¼šä½¿ç”¨å¯¹æ¯”è„šæœ¬**
```bash
python compare_robust_methods.py
```

### Q6: å›¾è¡¨ä¸­æ–‡æ˜¾ç¤ºå¼‚å¸¸ï¼Ÿ

**A:** è¿è¡Œå­—ä½“åˆå§‹åŒ–ï¼š

```python
from mplfonts.bin.cli import init
init()  # é¦–æ¬¡è¿è¡Œï¼Œè‡ªåŠ¨ä¸‹è½½ä¸­æ–‡å­—ä½“
```

å¦‚æœè¿˜ä¸è¡Œï¼š
```python
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows
# æˆ–
plt.rcParams['font.sans-serif'] = ['PingFang SC']  # Mac
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è¿‡æ‹Ÿåˆå¯è§†åŒ–æŒ‡å—](overfitting_visualization_guide.md) - è¯¦ç»†çš„APIæ–‡æ¡£
- [é²æ£’AdaBoostæŒ‡å—](robust_adaboost_guide.md) - æ”¹è¿›æ–¹æ³•è¯´æ˜
- [ç‰¹å¾é‡è¦æ€§æŒ‡å—](feature_importance_guide.md) - ç‰¹å¾åˆ†æ
- [è¯„ä¼°æŒ‡å—](evaluation_guide.md) - æ€§èƒ½è¯„ä¼°è¯¦è§£

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### æ¨èå·¥ä½œæµ

```bash
# ç¬¬1æ­¥ï¼šå¿«é€Ÿè¯Šæ–­ï¼ˆ5åˆ†é’Ÿï¼‰
python visualize_overfitting.py

# ç¬¬2æ­¥ï¼šæŸ¥çœ‹æŠ¥å‘Šï¼Œç¡®å®šé—®é¢˜
# - æ˜¯å¦è¿‡æ‹Ÿåˆï¼Ÿ
# - æœ€ä½³å¼±å­¦ä¹ å™¨æ•°é‡ï¼Ÿ
# - æ˜¯å¦éœ€è¦æ”¹è¿›ï¼Ÿ

# ç¬¬3æ­¥ï¼šå¦‚æœè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œå°è¯•é²æ£’æ–¹æ³•ï¼ˆ10åˆ†é’Ÿï¼‰
python demo_robust.py

# ç¬¬4æ­¥ï¼šè¯¦ç»†åˆ†æï¼ˆå¯é€‰ï¼‰
python train_with_noise_track.py
```

### è®ºæ–‡/æŠ¥å‘Šæ’°å†™

```python
# ç”Ÿæˆæ‰€æœ‰éœ€è¦çš„å›¾è¡¨
configs = {
    "baseline": {"noise": 0},
    "noise_5": {"noise": 0.05},
    "noise_10": {"noise": 0.10},
}

for name, config in configs.items():
    X_train, X_test, y_train, y_test, _, _ = prepare_data(
        noise_ratio=config["noise"]
    )
    
    visualize_overfitting_process(
        X_train, y_train, X_test, y_test,
        base_estimator=DecisionTreeClassifier(max_depth=1),
        n_estimators_list=[1, 5, 10, 20, 30, 40, 50, 75, 100],
        learning_rate=0.5,
        save_path=f'paper_figures/{name}.pdf'
    )
```

### æ¨¡å‹è°ƒä¼˜

```python
# ç¬¬1æ­¥ï¼šæ‰¾æœ€ä½³é…ç½®
results = visualize_overfitting_process(...)
best_n = results['n_estimators'][np.argmax(results['test_accuracy'])]

# ç¬¬2æ­¥ï¼šä½¿ç”¨æœ€ä½³é…ç½®
clf = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=best_n,
    learning_rate=0.5
)
clf.fit(X_train, y_train)

# ç¬¬3æ­¥ï¼šæœ€ç»ˆè¯„ä¼°
test_score = clf.score(X_test, y_test)
print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
```

---

**æœ€åæ›´æ–°ï¼š** 2024å¹´  
**ç»´æŠ¤è€…ï¼š** MLé¡¹ç›®ç»„


