import json
import os

import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split


def prepare_data(noise_ratio=0.05, test_size=0.2, random_state=42):
    """
    ä¸‹è½½ MNISTï¼Œå¹¶æŒ‰æŒ‡å®šæ¯”ä¾‹æ·»åŠ æ ‡ç­¾å™ªå£°ã€‚
    è‡ªåŠ¨è¿”å›ï¼š
        - X_train, X_test
        - y_train (å«å™ªå£°) , y_test
        - train_noise_indices  (è®­ç»ƒé›†å†…éƒ¨å™ªå£°ç´¢å¼•)
        - train_clean_indices  (è®­ç»ƒé›†å†…éƒ¨å¹²å‡€ç´¢å¼•)
    è‹¥ noise_ratio=0ï¼Œåˆ™è¿”å›å®Œå…¨å¹²å‡€çš„æ•°æ®ã€‚

    Parameters
    ----------
    noise_ratio : float
        å™ªå£°æ¯”ä¾‹ï¼ˆ0 ~ 1ï¼‰ï¼Œè¡¨ç¤ºæ ‡ç­¾å™ªå£°çš„æ¯”ä¾‹ã€‚
        è‹¥ä¸º 0ï¼Œåˆ™ä¸æ·»åŠ æ ‡ç­¾å™ªå£°ã€‚

    test_size : float
        train_test_split çš„æµ‹è¯•é›†å æ¯”

    random_state : int
        éšæœºç§å­

    Returns
    -------
    X_train, X_test : ndarray
    y_train, y_test : ndarray
    train_noise_indices : ndarray (è®­ç»ƒé›†å†…éƒ¨çš„å™ªå£°æ ·æœ¬ä½ç½®)
    train_clean_indices : ndarray
    """

    print("Downloading MNIST...")
    X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
    y = y.astype(np.int64)
    X = X / 255.0

    n_samples = len(y)

    # -----------------------------------------
    # Case 1: ä¸æ·»åŠ å™ªå£°ï¼Œè¿”å›åŸå§‹æ•°æ®
    # -----------------------------------------
    if noise_ratio <= 0:
        print("No noise added, returning clean dataset.")

        X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
            X, y, np.arange(n_samples), test_size=test_size, random_state=random_state
        )

        # è®­ç»ƒé›†å…¨éƒ¨æ˜¯ clean
        train_noise_indices = np.array([], dtype=int)
        train_clean_indices = np.arange(len(y_train))

        return (
            X_train,
            X_test,
            y_train,
            y_test,
            train_noise_indices,
            train_clean_indices,
        )

    # Case 2: æ·»åŠ å™ªå£°
    n_noisy = int(n_samples * noise_ratio)
    rng = np.random.default_rng(random_state)

    noise_indices = rng.choice(n_samples, n_noisy, replace=False)

    y_noisy = y.copy()
    y_noisy[noise_indices] = rng.integers(0, 10, size=n_noisy)

    print(f"Injected label noise: {noise_ratio * 100:.1f}% ({n_noisy} samples)")

    # train/test splitï¼Œä¿ç•™åŸå§‹ç´¢å¼•
    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
        X, y_noisy, np.arange(n_samples), test_size=test_size, random_state=random_state
    )

    # è®¡ç®—è®­ç»ƒé›†å†…éƒ¨å™ªå£°ä½ç½®
    train_noise_mask = np.isin(train_idx, noise_indices)
    train_noise_indices = np.where(train_noise_mask)[0]
    train_clean_indices = np.where(~train_noise_mask)[0]

    print(f"Training set noise samples = {len(train_noise_indices)}")

    return (X_train, X_test, y_train, y_test, train_noise_indices, train_clean_indices)


def load_config(config_path):
    with open(config_path, "r") as f:
        return json.load(f)


def build_experiment(config_path):
    config = load_config(config_path)

    exp_name = config["experiment"]["name"]
    exp_dir = os.path.join("experiments", exp_name)

    # === è‡ªåŠ¨åˆ›å»ºç›®å½•ç»“æ„ ===
    ckpt_dir = os.path.join(exp_dir, "checkpoints")
    result_dir = os.path.join(exp_dir, "results")
    os.makedirs(ckpt_dir, exist_ok=True)
    os.makedirs(result_dir, exist_ok=True)

    print(f"ğŸ“ å®éªŒç›®å½•å·²åˆ›å»º: {exp_dir}")

    # === 1. æ•°æ®å‡†å¤‡ ===
    data_cfg = config["data"]
    (
        X_train,
        X_test,
        y_train,
        y_test,
        noise_idx,
        clean_idx,
    ) = prepare_data(
        noise_ratio=data_cfg["noise_ratio"],
        test_size=data_cfg["test_size"],
        random_state=data_cfg["random_state"],
    )

    # === 2. æ„é€  Monitorï¼ˆè‡ªåŠ¨æ‹¼ checkpoint å‰ç¼€ï¼‰ ===
    monitor_cfg = config["monitor"]
    checkpoint_prefix = os.path.join(ckpt_dir, "round")

    monitor = BoostMonitor(
        noise_indices=noise_idx,
        clean_indices=clean_idx,
        is_data_noisy=monitor_cfg["is_data_noisy"],
        checkpoint_interval=monitor_cfg["checkpoint_interval"],
        checkpoint_prefix=checkpoint_prefix,
    )

    # === 3. æ„é€ æ¨¡å‹ ===
    model_cfg = config["model"]
    base = DecisionTreeClassifier(**model_cfg["estimator"])

    clf = AdaBoostClfWithMonitor(
        _monitor=monitor,
        X_val=X_test,
        y_val=y_test,
        estimator=base,
        n_estimators=model_cfg["n_estimators"],
        learning_rate=model_cfg["learning_rate"],
        random_state=model_cfg["random_state"],
    )

    # è¿”å›è·¯å¾„ä¾›ä¿å­˜
    result_csv = os.path.join(result_dir, "final_results.csv")

    return (
        clf,
        monitor,
        (X_train, X_test, y_train, y_test, noise_idx, clean_idx),
        result_csv,
    )
